{
 "cells": [
  {
   "cell_type": "code",
   "def extract_method(\n",
   "    snippet: str,\n",
   "    file_path: str,\n",
   "    method_name: str,\n",
   "    project_name: str = \"../../sweepai\"\n",
   ") -> str:\n",
     "output_type": "stream",
     "text": [
      "28 64\n"
     ]
    }
   ],
   "source": [
    "import rope.base.project\n",
    "from rope.refactor.extract import ExtractMethod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 64\n",
      "--- a/test.py\n",
      "+++ b/test.py\n",
      "@@ -1,5 +1,8 @@\n",
      " def a_func():\n",
      "     a = 1\n",
      "+    b, c = helper(a)\n",
      "+    print(b, c)\n",
      "+def helper(a):\n",
      "     b = 2 * a\n",
      "     c = a * 2 + b * 3\n",
      "-    print(b, c)+    return b, c\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myproject = rope.base.project.Project('src')\n",
    "\n",
    "myresource = myproject.get_resource('test.py')\n",
    "print(myresource.read().find(\"b\"), myresource.read().find(\"print\"))\n",
    "\n",
    "extractor = ExtractMethod(myproject, myresource, 28, 64)\n",
    "change_set = extractor.get_changes(\"helper\")\n",
    "for change in change_set.changes:\n",
    "    print(change.get_description())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import json\n",
      "import re\n",
      "import time\n",
      "from functools import lru_cache\n",
      "from typing import Generator, List\n",
      "\n",
      "import numpy as np\n",
      "import replicate\n",
      "import requests\n",
      "from deeplake.core.vectorstore.deeplake_vectorstore import (  # pylint: disable=import-error\n",
      "    VectorStore,\n",
      ")\n",
      "from loguru import logger\n",
      "from redis import Redis\n",
      "from sentence_transformers import SentenceTransformer  # pylint: disable=import-error\n",
      "from tqdm import tqdm\n",
      "\n",
      "from sweepai.config.client import SweepConfig\n",
      "from sweepai.config.server import (\n",
      "    BATCH_SIZE,\n",
      "    HUGGINGFACE_TOKEN,\n",
      "    HUGGINGFACE_URL,\n",
      "    REDIS_URL,\n",
      "    REPLICATE_API_KEY,\n",
      "    REPLICATE_DEPLOYMENT_URL,\n",
      "    SENTENCE_TRANSFORMERS_MODEL,\n",
      "    VECTOR_EMBEDDING_SOURCE,\n",
      ")\n",
      "from sweepai.core.entities import Snippet\n",
      "from sweepai.core.lexical_search import prepare_index_from_snippets, search_index\n",
      "from sweepai.core.repo_parsing_utils import repo_to_chunks\n",
      "from sweepai.logn import file_cache\n",
      "from sweepai.utils.event_logger import posthog\n",
      "from sweepai.utils.hash import hash_sha256\n",
      "from sweepai.utils.scorer import compute_score, get_scores\n",
      "\n",
      "from ..utils.github_utils import ClonedRepo\n",
      "\n",
      "MODEL_DIR = \"/tmp/cache/model\"\n",
      "DEEPLAKE_DIR = \"/tmp/cache/\"\n",
      "timeout = 60 * 60  # 30 minutes\n",
      "CACHE_VERSION = \"v1.0.13\"\n",
      "MAX_FILES = 500\n",
      "\n",
      "redis_client = Redis.from_url(REDIS_URL)\n",
      "\n",
      "\n",
      "def download_models():\n",
      "    from sentence_transformers import (  # pylint: disable=import-error\n",
      "        SentenceTransformer,\n",
      "    )\n",
      "\n",
      "    model = SentenceTransformer(SENTENCE_TRANSFORMERS_MODEL, cache_folder=MODEL_DIR)\n",
      "\n",
      "\n",
      "def init_deeplake_vs(repo_name: str) -> VectorStore:\n",
      "    deeplake_repo_path = f\"mem://{int(time.time())}{repo_name}\"\n",
      "    deeplake_vector_store = VectorStore(\n",
      "        path=deeplake_repo_path, read_only=False, overwrite=False\n",
      "    )\n",
      "    return deeplake_vector_store\n",
      "\n",
      "\n",
      "def parse_collection_name(name: str) -> str:\n",
      "    # Replace any non-alphanumeric characters with hyphens\n",
      "    name = re.sub(r\"[^\\w-]\", \"--\", name)\n",
      "    # Ensure the name is between 3 and 63 characters and starts/ends with alphanumeric\n",
      "    name = re.sub(r\"^(-*\\w{0,61}\\w)-*$\", r\"\\1\", name[:63].ljust(3, \"x\"))\n",
      "    return name\n",
      "\n",
      "\n",
      "def embed_huggingface(texts: List[str]) -> List[np.ndarray]:\n",
      "    \"\"\"Embeds a list of texts using Hugging Face's API.\"\"\"\n",
      "    for i in range(3):\n",
      "        try:\n",
      "            headers = {\n",
      "                \"Authorization\": f\"Bearer {HUGGINGFACE_TOKEN}\",\n",
      "                \"Content-Type\": \"application/json\",\n",
      "            }\n",
      "            response = requests.post(\n",
      "                HUGGINGFACE_URL, headers=headers, json={\"inputs\": texts}\n",
      "            )\n",
      "            return response.json()[\"embeddings\"]\n",
      "        except requests.exceptions.RequestException as e:\n",
      "            logger.exception(\n",
      "                f\"Error occurred when sending request to Hugging Face endpoint: {e}\"\n",
      "            )\n",
      "\n",
      "\n",
      "def embed_replicate(texts: List[str]) -> List[np.ndarray]:\n",
      "    client = replicate.Client(api_token=REPLICATE_API_KEY)\n",
      "    deployment = client.deployments.get(REPLICATE_DEPLOYMENT_URL)\n",
      "    e = None\n",
      "    for i in range(3):\n",
      "        try:\n",
      "            prediction = deployment.predictions.create(\n",
      "                input={\"text_batch\": json.dumps(texts)}, timeout=60\n",
      "            )\n",
      "            prediction.wait()\n",
      "            outputs = prediction.output\n",
      "            break\n",
      "        except Exception:\n",
      "            logger.exception(f\"Replicate timeout: {e}\")\n",
      "    else:\n",
      "        raise Exception(f\"Replicate timeout\")\n",
      "    return [output[\"embedding\"] for output in outputs]\n",
      "\n",
      "\n",
      "@lru_cache(maxsize=64)\n",
      "def embed_texts(texts: tuple[str]):\n",
      "    logger.info(\n",
      "        f\"Computing embeddings for {len(texts)} texts using {VECTOR_EMBEDDING_SOURCE}...\"\n",
      "    )\n",
      "    match VECTOR_EMBEDDING_SOURCE:\n",
      "        case \"sentence-transformers\":\n",
      "            sentence_transformer_model = SentenceTransformer(\n",
      "                SENTENCE_TRANSFORMERS_MODEL, cache_folder=MODEL_DIR\n",
      "            )\n",
      "            vector = sentence_transformer_model.encode(\n",
      "                texts, show_progress_bar=True, batch_size=BATCH_SIZE\n",
      "            )\n",
      "            return vector\n",
      "        case \"openai\":\n",
      "            import openai\n",
      "\n",
      "            embeddings = []\n",
      "            for batch in tqdm(chunk(texts, batch_size=BATCH_SIZE), disable=False):\n",
      "                try:\n",
      "                    response = openai.Embedding.create(\n",
      "                        input=batch, model=\"text-embedding-ada-002\"\n",
      "                    )\n",
      "                    embeddings.extend([r[\"embedding\"] for r in response[\"data\"]])\n",
      "                except SystemExit:\n",
      "                    raise SystemExit\n",
      "                except Exception:\n",
      "                    logger.exception(\"Failed to get embeddings for batch\")\n",
      "                    logger.error(f\"Failed to get embeddings for {batch}\")\n",
      "            return embeddings\n",
      "        case \"huggingface\":\n",
      "            if HUGGINGFACE_URL and HUGGINGFACE_TOKEN:\n",
      "                embeddings = []\n",
      "                for batch in tqdm(chunk(texts, batch_size=BATCH_SIZE), disable=False):\n",
      "                    embeddings.extend(embed_huggingface(texts))\n",
      "                return embeddings\n",
      "            else:\n",
      "                raise Exception(\"Hugging Face URL and token not set\")\n",
      "        case \"replicate\":\n",
      "            if REPLICATE_API_KEY:\n",
      "                embeddings = []\n",
      "                for batch in tqdm(chunk(texts, batch_size=BATCH_SIZE)):\n",
      "                    embeddings.extend(embed_replicate(batch))\n",
      "                return embeddings\n",
      "            else:\n",
      "                raise Exception(\"Replicate URL and token not set\")\n",
      "        case _:\n",
      "            raise Exception(\"Invalid vector embedding mode\")\n",
      "    logger.info(\n",
      "        f\"Computed embeddings for {len(texts)} texts using {VECTOR_EMBEDDING_SOURCE}\"\n",
      "    )\n",
      "\n",
      "\n",
      "def embedding_function(texts: list[str]):\n",
      "    # For LRU cache to work\n",
      "    return embed_texts(tuple(texts))\n",
      "\n",
      "\n",
      "def get_deeplake_vs_from_repo(\n",
      "    cloned_repo: ClonedRepo,\n",
      "    sweep_config: SweepConfig = SweepConfig(),\n",
      "):\n",
      "    deeplake_vs = None\n",
      "\n",
      "    repo_full_name = cloned_repo.repo_full_name\n",
      "    repo = cloned_repo.repo\n",
      "    commits = repo.get_commits()\n",
      "    commit_hash = commits[0].sha\n",
      "\n",
      "    logger.info(f\"Downloading repository and indexing for {repo_full_name}...\")\n",
      "    start = time.time()\n",
      "    logger.info(\"Recursively getting list of files...\")\n",
      "    snippets, file_list = repo_to_chunks(cloned_repo.cache_dir, sweep_config)\n",
      "    logger.info(f\"Found {len(snippets)} snippets in repository {repo_full_name}\")\n",
      "    # prepare lexical search\n",
      "    index = prepare_index_from_snippets(\n",
      "        snippets, len_repo_cache_dir=len(cloned_repo.cache_dir) + 1\n",
      "    )\n",
      "    logger.print(\"Prepared index from snippets\")\n",
      "    # scoring for vector search\n",
      "    files_to_scores = {}\n",
      "    score_factors = []\n",
      "    for file_path in tqdm(file_list):\n",
      "        if not redis_client:\n",
      "            score_factor = compute_score(\n",
      "                file_path[len(cloned_repo.cache_dir) + 1 :], cloned_repo.git_repo\n",
      "            )\n",
      "            score_factors.append(score_factor)\n",
      "            continue\n",
      "        cache_key = hash_sha256(file_path) + CACHE_VERSION\n",
      "        try:\n",
      "            cache_value = redis_client.get(cache_key)\n",
      "        except Exception as e:\n",
      "            logger.exception(e)\n",
      "            cache_value = None\n",
      "        if cache_value is not None:\n",
      "            score_factor = json.loads(cache_value)\n",
      "            score_factors.append(score_factor)\n",
      "        else:\n",
      "            score_factor = compute_score(\n",
      "                file_path[len(cloned_repo.cache_dir) + 1 :], cloned_repo.git_repo\n",
      "            )\n",
      "            score_factors.append(score_factor)\n",
      "            redis_client.set(cache_key, json.dumps(score_factor))\n",
      "    # compute all scores\n",
      "    all_scores = get_scores(score_factors)\n",
      "    files_to_scores = {\n",
      "        file_path: score for file_path, score in zip(file_list, all_scores)\n",
      "    }\n",
      "    logger.info(f\"Found {len(file_list)} files in repository {repo_full_name}\")\n",
      "\n",
      "    documents = []\n",
      "    metadatas = []\n",
      "    ids = []\n",
      "    for snippet in snippets:\n",
      "        documents.append(snippet.get_snippet(add_ellipsis=False, add_lines=False))\n",
      "        metadata = {\n",
      "            \"file_path\": snippet.file_path[len(cloned_repo.cache_dir) + 1 :],\n",
      "            \"start\": snippet.start,\n",
      "            \"end\": snippet.end,\n",
      "            \"score\": files_to_scores[snippet.file_path],\n",
      "        }\n",
      "        metadatas.append(metadata)\n",
      "        gh_file_path = snippet.file_path[len(\"repo/\") :]\n",
      "        ids.append(f\"{gh_file_path}:{snippet.start}:{snippet.end}\")\n",
      "    logger.info(f\"Getting list of all files took {time.time() - start}\")\n",
      "    logger.info(f\"Received {len(documents)} documents from repository {repo_full_name}\")\n",
      "    collection_name = parse_collection_name(repo_full_name)\n",
      "\n",
      "    deeplake_vs = deeplake_vs or compute_deeplake_vs(\n",
      "        collection_name, documents, ids, metadatas, commit_hash\n",
      "    )\n",
      "\n",
      "    return deeplake_vs, index, len(documents)\n",
      "\n",
      "\n",
      "def compute_deeplake_vs(collection_name: str, documents: List[str], ids: List[str], metadatas: List[Dict[str, Any]], sha: str) -> VectorStore:\n",
      "    if len(documents) > 0:\n",
      "        logger.info(f\"Computing embeddings with {VECTOR_EMBEDDING_SOURCE}...\")\n",
      "        # Check cache here for all documents\n",
      "        embeddings = [None] * len(documents)\n",
      "        if redis_client:\n",
      "            cache_keys = [\n",
      "                hash_sha256(doc)\n",
      "                + SENTENCE_TRANSFORMERS_MODEL\n",
      "                + VECTOR_EMBEDDING_SOURCE\n",
      "                + CACHE_VERSION\n",
      "                for doc in documents\n",
      "            ]\n",
      "            cache_values = redis_client.mget(cache_keys)\n",
      "            for idx, value in enumerate(cache_values):\n",
      "                if value is not None:\n",
      "                    arr = json.loads(value)\n",
      "                    if isinstance(arr, list):\n",
      "                        embeddings[idx] = np.array(arr, dtype=np.float32)\n",
      "\n",
      "        logger.info(\n",
      "            f\"Found {len([x for x in embeddings if x is not None])} embeddings in cache\"\n",
      "        )\n",
      "        indices_to_compute = [idx for idx, x in enumerate(embeddings) if x is None]\n",
      "        documents_to_compute = [documents[idx] for idx in indices_to_compute]\n",
      "\n",
      "        logger.info(f\"Computing {len(documents_to_compute)} embeddings...\")\n",
      "        computed_embeddings = embedding_function(documents_to_compute)\n",
      "        logger.info(f\"Computed {len(computed_embeddings)} embeddings\")\n",
      "\n",
      "        for idx, embedding in zip(indices_to_compute, computed_embeddings):\n",
      "            embeddings[idx] = embedding\n",
      "\n",
      "        embeddings = convert_embeddings(embeddings, documents)\n",
      "\n",
      "        deeplake_vs = init_deeplake_vs(collection_name)\n",
      "        deeplake_vs.add(text=ids, embedding=embeddings, metadata=metadatas)\n",
      "        logger.info(\"Added embeddings to cache\")\n",
      "        if redis_client and len(documents_to_compute) > 0:\n",
      "            logger.info(f\"Updating cache with {len(computed_embeddings)} embeddings\")\n",
      "            cache_keys = [\n",
      "                hash_sha256(doc)\n",
      "                + SENTENCE_TRANSFORMERS_MODEL\n",
      "                + VECTOR_EMBEDDING_SOURCE\n",
      "                + CACHE_VERSION\n",
      "                for doc in documents_to_compute\n",
      "            ]\n",
      "            redis_client.mset(\n",
      "                {\n",
      "                    key: json.dumps(\n",
      "                        embedding.tolist()\n",
      "                        if isinstance(embedding, np.ndarray)\n",
      "                        else embedding\n",
      "                    )\n",
      "                    for key, embedding in zip(cache_keys, computed_embeddings)\n",
      "                }\n",
      "            )\n",
      "        return deeplake_vs\n",
      "    else:\n",
      "        logger.error(\"No documents found in repository\")\n",
      "        return deeplake_vs\n",
      "\n",
      "def convert_embeddings(embeddings: List[np.ndarray], documents: List[str]) -> np.ndarray:\n",
      "    try:\n",
      "        embeddings = np.array(embeddings, dtype=np.float32)\n",
      "    except SystemExit:\n",
      "        raise SystemExit\n",
      "    except:\n",
      "        logger.exception(\n",
      "            \"Failed to convert embeddings to numpy array, recomputing all of them\"\n",
      "        )\n",
      "        embeddings = embedding_function(documents)\n",
      "        embeddings = np.array(embeddings, dtype=np.float32)\n",
      "    return embeddings\n",
      "\n",
      "\n",
      "# Only works on functions without side effects\n",
      "@file_cache(ignore_params=[\"cloned_repo\", \"sweep_config\", \"token\"])\n",
      "def get_relevant_snippets(\n",
      "    cloned_repo: ClonedRepo,\n",
      "    query: str,\n",
      "    username: str | None = None,\n",
      "    sweep_config: SweepConfig = SweepConfig(),\n",
      "    lexical=True,\n",
      "):\n",
      "    repo_name = cloned_repo.repo_full_name\n",
      "    installation_id = cloned_repo.installation_id\n",
      "    logger.info(\"Getting query embedding...\")\n",
      "    query_embedding = embedding_function([query])  # pylint: disable=no-member\n",
      "    logger.info(\"Starting search by getting vector store...\")\n",
      "    deeplake_vs, lexical_index, num_docs = get_deeplake_vs_from_repo(\n",
      "        cloned_repo, sweep_config=sweep_config\n",
      "    )\n",
      "    content_to_lexical_score = search_index(query, lexical_index)\n",
      "    logger.info(f\"Found {len(content_to_lexical_score)} lexical results\")\n",
      "    logger.info(f\"Searching for relevant snippets... with {num_docs} docs\")\n",
      "    results = {\"metadata\": [], \"text\": []}\n",
      "    try:\n",
      "        results = deeplake_vs.search(embedding=query_embedding, k=num_docs)\n",
      "    except SystemExit:\n",
      "        raise SystemExit\n",
      "    except Exception:\n",
      "        logger.exception(\"Exception occurred while fetching relevant snippets\")\n",
      "    logger.info(\"Fetched relevant snippets...\")\n",
      "    if len(results[\"text\"]) == 0:\n",
      "        logger.info(f\"Results query {query} was empty\")\n",
      "        logger.info(f\"Results: {results}\")\n",
      "        if username is None:\n",
      "            username = \"anonymous\"\n",
      "        posthog.capture(\n",
      "            username,\n",
      "            \"failed\",\n",
      "            {\n",
      "                \"reason\": \"Results query was empty\",\n",
      "                \"repo_name\": repo_name,\n",
      "                \"installation_id\": installation_id,\n",
      "                \"query\": query,\n",
      "            },\n",
      "        )\n",
      "        return []\n",
      "    metadatas = results[\"metadata\"]\n",
      "    code_scores = [metadata[\"score\"] for metadata in metadatas]\n",
      "    lexical_scores = []\n",
      "    for metadata in metadatas:\n",
      "        key = f\"{metadata['file_path']}:{str(metadata['start'])}:{str(metadata['end'])}\"\n",
      "        if key in content_to_lexical_score:\n",
      "            lexical_scores.append(content_to_lexical_score[key])\n",
      "        else:\n",
      "            lexical_scores.append(0.3)\n",
      "    vector_scores = results[\"score\"]\n",
      "    combined_scores = [\n",
      "        code_score * 4\n",
      "        + vector_score\n",
      "        + lexical_score * 2.5  # increase weight of lexical search\n",
      "        for code_score, vector_score, lexical_score in zip(\n",
      "            code_scores, vector_scores, lexical_scores\n",
      "        )\n",
      "    ]\n",
      "    combined_list = list(zip(combined_scores, metadatas))\n",
      "    sorted_list = sorted(combined_list, key=lambda x: x[0], reverse=True)\n",
      "    sorted_metadatas = [metadata for _, metadata in sorted_list]\n",
      "    relevant_paths = [metadata[\"file_path\"] for metadata in sorted_metadatas]\n",
      "    logger.info(\"Relevant paths: {}\".format(relevant_paths[:5]))\n",
      "    return [\n",
      "        Snippet(\n",
      "            content=\"\",\n",
      "            start=metadata[\"start\"],\n",
      "            end=metadata[\"end\"],\n",
      "            file_path=file_path,\n",
      "        )\n",
      "        for metadata, file_path in zip(sorted_metadatas, relevant_paths)\n",
      "    ][:num_docs]\n",
      "\n",
      "\n",
      "def chunk(texts: List[str], batch_size: int) -> Generator[List[str], None, None]:\n",
      "    \"\"\"\n",
      "    Split a list of texts into batches of a given size for embed_texts.\n",
      "\n",
      "    Args:\n",
      "    ----\n",
      "        texts (List[str]): A list of texts to be chunked into batches.\n",
      "        batch_size (int): The maximum number of texts in each batch.\n",
      "\n",
      "    Yields:\n",
      "    ------\n",
      "        Generator[List[str], None, None]: A generator that yields batches of texts as lists.\n",
      "\n",
      "    Example:\n",
      "    -------\n",
      "        texts = [\"text1\", \"text2\", \"text3\", \"text4\", \"text5\"]\n",
      "        batch_size = 2\n",
      "        for batch in chunk(texts, batch_size):\n",
      "            print(batch)\n",
      "        # Output:\n",
      "        # ['text1', 'text2']\n",
      "        # ['text3', 'text4']\n",
      "        # ['text5']\n",
      "    \"\"\"\n",
      "    texts = [text[:4096] if text else \" \" for text in texts]\n",
      "    for text in texts:\n",
      "        assert isinstance(text, str), f\"Expected str, got {type(text)}\"\n",
      "        assert len(text) <= 4096, f\"Expected text length <= 4096, got {len(text)}\"\n",
      "    for i in range(0, len(texts), batch_size):\n",
      "        yield texts[i : i + batch_size] if i + batch_size < len(texts) else texts[i:]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def extract_method(\n",
    "    snippet,\n",
    "    file_path,\n",
    "    method_name,\n",
    "    project_name=\"../../sweepai\"\n",
    "):\n",
    "    project = rope.base.project.Project(project_name)\n",
    "\n",
    "    resource = project.get_resource(file_path)\n",
    "    contents = resource.read()\n",
    "    start, end = contents.find(snippet), contents.find(snippet) + len(snippet)\n",
    "\n",
    "    extractor = ExtractMethod(project, resource, start, end)\n",
    "    change_set = extractor.get_changes(method_name, similar=True, global_=True)\n",
    "    for change in change_set.changes:\n",
    "        change.do()\n",
    "\n",
    "    result = resource.read()\n",
    "\n",
    "    for change in change_set.changes:\n",
    "        change.undo()\n",
    "    return result\n",
    "\n",
    "snippet = \"\"\"        try:\n",
    "            embeddings = np.array(embeddings, dtype=np.float32)\n",
    "        except SystemExit:\n",
    "            raise SystemExit\n",
    "        except:\n",
    "            logger.exception(\n",
    "                \"Failed to convert embeddings to numpy array, recomputing all of them\"\n",
    "            )\n",
    "            embeddings = embedding_function(documents)\n",
    "            embeddings = np.array(embeddings, dtype=np.float32)\"\"\"\n",
    "print(extract_method(snippet, \"core/vector_db.py\", \"convert_embeddings\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method _handle_job_set.<locals>.call of <rope.base.change.ChangeContents object at 0x7fdead84bdf0>>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "change.do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('sweepai-u_CIt3kb-py3.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25d341f3248a096a89b9dbf6eec8e41f63aed02f6ba059df22a49224e3e8f1b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
