# Our simple vector database implementation
**William Zeng** - March 12th, 2024

---
Classic vector databases worked great for us when we had a single index, but they became a pain we started managing multiple indices and frequent updates.

Our backend consists of a single docker image that serves hundreds of repositories, each with a few thousand files.

We started with Chroma as our vector database but decided to go with something self-managed. Here’s our implementation:

# link to redis_vector_db.py

You can use it by directly copy and pasting it into your codebase. The only dependencies are `numpy` and `redis`.

## Overview

This is the main loop that we use. 
```python
def openai_with_expo_backoff(batch: tuple[str]):
    # 0. If we don't have a redis client, just call openai
    if not redis_client:
        return openai_call_embedding(batch)
    # 1. Get embeddings from redis, using the hash of the text as the key
    embeddings: list[np.ndarray] = [None] * len(batch)
    cache_keys = [hash_text(text) + CACHE_VERSION for text in batch]
    try:
        for i, cache_value in enumerate(redis_client.mget(cache_keys)):
            if cache_value:
                embeddings[i] = np.array(json.loads(cache_value))
    except Exception as e:
        logger.exception(e)
    # 2. If we have all the embeddings, return them
    batch = [text for idx, text in enumerate(batch) if isinstance(embeddings[idx], type(None))]
    if len(batch) == 0:
        embeddings = np.array(embeddings)
        return embeddings
    # 3. If we don't have all the embeddings, call openai for the missing ones
    try:
        new_embeddings = openai_call_embedding(batch)
    except requests.exceptions.Timeout as e:
        logger.exception(f"Timeout error occured while embedding: {e}")
    except BadRequestError as e:
        try:
            # 4. If we get a BadRequestError, truncate the text and try again
            batch = [truncate_string_tiktoken(text) for text in batch] # truncation is slow, so we only do it if we have to
            new_embeddings = openai_call_embedding(batch)
        except Exception as e:
            logger.exception(e)
    # 5. Place the new embeddings in the correct position
    indices = [i for i, emb in enumerate(embeddings) if emb is None]
    for i, index in enumerate(indices):
        embeddings[index] = new_embeddings[i]
    # 6. Store the new embeddings in redis
    redis_client.mset(
        {
            cache_key: json.dumps(embedding.tolist())
            for cache_key, embedding in zip(cache_keys, embeddings)
        }
    )
    return np.array(embeddings)
```


Here’s a bit about why we designed our vector database this way.

Pros:

- Handle multiple different codebases with frequent updates
    - This architecture allows us to not require any indices. Typically in Pinecone you’d have repo 1, repo 2, and repo 3 under different namespaces https://docs.pinecone.io/reference/fetch. This makes a lot of sense when you a small amount of very large indices, but becomes cumbersome when you have many small indices. We’d also have to manage the updates as each codebase updates (every hour).
- Reliability/Developer experience
    - Depending on an external store introduces another dependency for us. We’d have to handle when Pinecone fails, and it’s not self-hostable. There’s also an overhead to learning how to use a new API and manage new infrastructure. We didn’t believe the cost was worth it in this case.

Cons:

- Speed
    - This approach is slow. We key our cache on the actual content’s SHA256 hash. A codebase doesn’t tend to a substantial percentage of it’s files day-to-day. So if we use the actual contents as a hash, we only have to re-embed the diffs. This lets us have a high cache hit ratio.
- Scale
    - This approach doesn’t scale well for indices with 1M+ embeddings. Fortunately for us that hasn’t been a problem yet. The codebases we work with usually do not exceed 30k files, and we have a chunks:doc ratio of about 1.5, leading to 45k files. This takes about 1 second to serve, and we own all of the infrastructure!