# How we cache slow-running function calls in Python
**William Zeng** - March 14th, 2024

---
We wrote a file cache - like `lru_cache` but caching in files instead of in memory. This has saved us hours of time while running our LLM benchmarks. We'd like to share it with our community as copy and pasteable code. Thanks to [Luke Jaggernauth](https://github.com/lukejagg) (former Sweep engineer) for building the initial version of this!

Here's the link:
https://github.com/sweepai/sweep/blob/main/docs/public/file_cache.py.

## Background
We spend a lot of time prompt engineering our "agents" at Sweep. Our agents take a set of input strings, format them as a prompt, and then extract the important output from them.
For example, to modify code we'll input the old code, relevant context, and instructions and then output the new code.

<div style={{ display: "flex", justifyContent: "center", alignItems: "center" }}>
    <img src="/assets/multi_llm_step.png" alt="Multiple llm steps being chained" style={{ height: "400px" }} />
</div>
We improve Sweep's performance on this pipeline every day. This typically involves tweaking a small part of our pipeline (like improving our planning algorithm), and then running the entire pipeline again. 
We often use pdb (python's native debugger) to set breakpoints and inspect the state of our prompts, input values, and parsing logic.
For example, we can check whether a certain string matches a regex that we inserted at runtime:
```shell
(Pdb) print(todays_date)
'2024-03-14'
(Pdb) re.match(r"^\d{4}-\d{2}-\d{2}$", todays_date)
<re.Match object; span=(0, 10), match='2024-03-14'>
```
This lets us debug at runtime with the actual data.

## A cached version of pdb
pdb works great, but we have to wait for the entire pipeline to run again.
We thought about whether it was possible to build a version of pdb that not only interrupted execution, but also cached the entire program state up to that point. 
Our time to hit that same bug could be cut from 10 minutes to 15 seconds (a 40x improvement).

In our opinion, we built something better.

LLM calls are unique in that they take a really long time, but their inputs and outputs are trivially cacheable. We can save a massive amount of time by caching the results of these calls.

### What's different from lru_cache?

lru_cache works well, but it doesn't support two key features.

First, we need to persist the cache between runs. lru_cache stores the results in-memory, which means that the next time you run the program the cache will be empty.
We considered Redis for persistence, but the simplest way to do this is to write to local storage.

Second, lru_cache doesn't support ignoring arguments that invalidate the cache. Sometimes we'll pass in our custom `chat_logger` which stores the chats for visualization and contains the current timestamp (invalidating the cache when it's serialized).
To counteract this we added ignore params, used like so: `file_cache(ignore_params=["chat_logger"])`.

## Implementation

The two methods `recursive_hash` and `file_cache` handle the majority of the caching logic.

Let's go over `recursive_hash` first. We want to stably hash objects, and this is [not natively supported in python](https://death.andgravity.com/stable-hashing). Here's an example:

```python
import hashlib
from cache import recursive_hash


class Obj:
    def __init__(self, name):
        self.name = name

obj = Obj("test")
print(recursive_hash(obj)) # -> this works fine
try:
    hashlib.md5(obj).hexdigest()
except Exception as e:
    print(e) # -> this doesn't work
```

gives us the error: `TypeError: object supporting the buffer API required`. Recursively hashing and joining the keys allows us to have a nearly unique hash for any object.

Then we have the `file_cache` method. This is a decorator that takes in a function and caches its results to a file. Here's an example:

```python
@file_cache()
def search_codebase(
    cloned_github_repo,
    query,
):
    # ... take a long time ...
    # ... llm agent logic to search through the codebase ...
    return top_results
```

Without the cache, just searching through the codebase with our LLM agent can take 5 minutes - way too long if we're not actually testing it. Instead anything that takes more than 1 second can be cached and reduced to deserialization time - basically instantaneous for search results.

Here's our main wrapper.

First we store our cache in `/tmp/file_cache`. This lets us remove the cache by simply deleting the directory (`rm -rf /tmp/file_cache`).
We can also selectively remove function calls using `rm -rf /tmp/file_cache/search_codebase*`.
```python
def wrapper(*args, **kwargs):
    cache_dir = "/tmp/file_cache"
    os.makedirs(cache_dir, exist_ok=True)
```

Then we store the function's arguments and create a hash based on the argument names, argument values, and function source code. We also remove ignored params.

This addresses another problem - we want to invalidate our cache under two conditions.

1. The arguments to the function change - handled by `recursive_hash`
2. The code changes
To handle 2. we can simply use `inspect.getsource(func)` to add the function's source code to the hash, invalidating the cache when the function changes.

```python
    args_names = func.__code__.co_varnames[: func.__code__.co_argcount]
    args_dict = dict(zip(args_names, args))

    # Remove ignored params
    kwargs_clone = kwargs.copy()
    for param in ignore_params:
        args_dict.pop(param, None)
        kwargs_clone.pop(param, None)

    # Create hash based on argument names, argument values, and function source code
    arg_hash = (
        recursive_hash(args_dict, ignore_params=ignore_params)
        + recursive_hash(kwargs_clone, ignore_params=ignore_params)
        + hash_code(inspect.getsource(func))
    )
    cache_file = os.path.join(
        cache_dir, f"{func.__module__}_{func.__name__}_{arg_hash}.pickle"
    )
```

Finally we have some simple logic that checks cache key existence and writes to the cache in the case of a cache miss.

```python
    try:
        # If cache exists, load and return it
        if os.path.exists(cache_file):
            if verbose:
                print("Used cache for function: " + func.__name__)
            with open(cache_file, "rb") as f:
                return pickle.load(f)
    except Exception:
        logger.info("Unpickling failed")

    # Otherwise, call the function and save its result to the cache
    result = func(*args, **kwargs)
    try:
        with open(cache_file, "wb") as f:
            pickle.dump(result, f)
    except Exception as e:
        logger.info(f"Pickling failed: {e}")
    return result
```

## Conclusion

We hope this code is useful to you. We've found it to be a massive time saver when debugging LLM calls. We'd love to hear your feedback!
We're also happy to accept contributions at https://github.com/sweepai/sweep.